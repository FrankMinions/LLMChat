{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c071786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import datetime\n",
    "import random\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "\n",
    "__file__ = os.path.abspath('file')\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(0)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "LOGDIR = \"./logs\"\n",
    "\n",
    "no_change_btn = gr.Button.update()\n",
    "enable_btn = gr.Button.update(interactive=True, visible=True)\n",
    "disable_btn = gr.Button.update(interactive=False)\n",
    "invisible_btn = gr.Button(interactive=False, visible=False)\n",
    "\n",
    "MODEL_MAPPING_PATH = {\n",
    "    \"Qwen-7B-Chat\": \"\",\n",
    "    \"LLaMA-2-7B-Chat\": \"\"\n",
    "}\n",
    "\n",
    "tokenizer = None\n",
    "model = None\n",
    "config = None\n",
    "\n",
    "\n",
    "models = [\"LLaMA-2-7B-Chat\", \"LLaMA-2-13B-Chat\", \"LLaMA-2-70B-Chat\", \"ChatGLM2-6B\", \n",
    "          \"Qwen-7B-Chat\", \"Qwen-14B-Chat\", \"Baichuan2-7B-Chat\", \"Baichuan2-13B-Chat\"]\n",
    "\n",
    "\n",
    "def load_model(model_name: str):\n",
    "    global tokenizer\n",
    "    global model\n",
    "    global config\n",
    "    state = None\n",
    "    model_path = MODEL_MAPPING_PATH[model_name]\n",
    "    if model_name.lower().startswith(\"llama\"):\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        model = LlamaForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map=\"auto\")\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map=\"auto\")\n",
    "        if model_name.lower().startswith(\"qwen\"):\n",
    "            from transformers.generation import GenerationConfig\n",
    "            config = GenerationConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "        elif model_name.lower().startswith(\"baichuan\"):\n",
    "            from transformers.generation.utils import GenerationConfig\n",
    "            config = GenerationConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "        else:\n",
    "            pass\n",
    "    return (state, [], \"\") + (enable_btn,) * 2\n",
    "    \n",
    "\n",
    "def llama_chat(prompt, history, temperature, top_p, max_new_tokens):\n",
    "    if not history:\n",
    "        history = []\n",
    "    generation_config = dict(\n",
    "        temperature=temperature,\n",
    "        top_k=0,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        tokenized_data = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        generation_output = model.generate(\n",
    "            input_ids=tokenized_data[\"input_ids\"].to(device),\n",
    "            attention_mask=tokenized_data['attention_mask'].to(device),\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            **generation_config)\n",
    "        response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "    history.append((prompt, response))\n",
    "    return (history, history,) + (enable_btn,) * 2\n",
    "\n",
    "\n",
    "def _parse_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if line != \"\"]\n",
    "    count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"```\" in line:\n",
    "            count += 1\n",
    "            items = line.split(\"`\")\n",
    "            if count % 2 == 1:\n",
    "                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n",
    "            else:\n",
    "                lines[i] = f\"<br></code></pre>\"\n",
    "        else:\n",
    "            if i > 0:\n",
    "                if count % 2 == 1:\n",
    "                    line = line.replace(\"`\", r\"\\`\")\n",
    "                    line = line.replace(\"<\", \"&lt;\")\n",
    "                    line = line.replace(\">\", \"&gt;\")\n",
    "                    line = line.replace(\" \", \"&nbsp;\")\n",
    "                    line = line.replace(\"*\", \"&ast;\")\n",
    "                    line = line.replace(\"_\", \"&lowbar;\")\n",
    "                    line = line.replace(\"-\", \"&#45;\")\n",
    "                    line = line.replace(\".\", \"&#46;\")\n",
    "                    line = line.replace(\"!\", \"&#33;\")\n",
    "                    line = line.replace(\"(\", \"&#40;\")\n",
    "                    line = line.replace(\")\", \"&#41;\")\n",
    "                    line = line.replace(\"$\", \"&#36;\")\n",
    "                lines[i] = \"<br>\" + line\n",
    "    text = \"\".join(lines)\n",
    "    return text\n",
    "    \n",
    "    \n",
    "def qwen_chat(prompt, history, temperature, top_p, max_new_tokens):\n",
    "    if not history:\n",
    "        history = []\n",
    "    full_response = \"\"\n",
    "    config.temperature = temperature\n",
    "    config.top_p = top_p\n",
    "    config.max_new_tokens = max_new_tokens\n",
    "    for response in model.chat_stream(tokenizer, prompt, history=history, generation_config=config):\n",
    "        full_response = _parse_text(response)\n",
    "    history.append((prompt, full_response)) \n",
    "    return (history, history,) + (enable_btn,) * 2\n",
    "\n",
    "\n",
    "def baichuan_chat(prompt, history, temperature, top_p, max_new_tokens):\n",
    "    if not history:\n",
    "        history = []\n",
    "    history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    full_response = \"\"\n",
    "    config.temperature = temperature\n",
    "    config.top_p = top_p\n",
    "    config.max_new_tokens = max_new_tokens\n",
    "    for response in model.chat(tokenizer, history, stream=True, generation_config=config):\n",
    "        full_response = response\n",
    "    history.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "    return (history, history,) + (enable_btn,) * 2\n",
    "    \n",
    "\n",
    "def get_conv_log_filename():\n",
    "    t = datetime.datetime.now()\n",
    "    name = os.path.join(LOGDIR, f\"{t.year}-{t.month:02d}-{t.day:02d}-conv.json\")\n",
    "    return name\n",
    "\n",
    "\n",
    "def vote_last_response(state, vote_type, model_selector, request: gr.Request):\n",
    "    with open(get_conv_log_filename(), mode=\"a\", encoding='utf-8') as f:\n",
    "        data = {\n",
    "            \"tstamp\": round(time.time(), 4),\n",
    "            \"type\": vote_type,\n",
    "            \"model\": model_selector,\n",
    "            \"state\": state,\n",
    "            \"ip\": request.client.host,\n",
    "        }\n",
    "        f.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "        \n",
    "def upvote_last_response(state, model_selector, request:gr.Request):\n",
    "    vote_last_response(state[-1], \"upvote\", model_selector, request)\n",
    "    return (state, state,) + (disable_btn,) * 2\n",
    "\n",
    "\n",
    "def downvote_last_response(state, model_selector, request:gr.Request):\n",
    "    vote_last_response(state[-1], \"downvote\", model_selector, request)\n",
    "    return (state, state,) + (disable_btn,) * 2\n",
    "\n",
    "    \n",
    "def clear_history(request: gr.Request):\n",
    "    state = None\n",
    "    return (state, [], \"\") + (enable_btn,) * 2\n",
    "\n",
    "\n",
    "def chat(model_selector, prompt, history, temperature, top_p, max_new_tokens):\n",
    "    \n",
    "    if model_selector.lower().startswith(\"llama\"):\n",
    "        response = llama_chat(prompt, history, temperature, top_p, max_new_tokens)\n",
    "    elif model_selector.lower().startswith(\"qwen\"):\n",
    "        response = qwen_chat(prompt, history, temperature, top_p, max_new_tokens)\n",
    "    elif model_selector.lower().startswith(\"baichuan\"):\n",
    "        response = baichuan_chat(prompt, history, temperature, top_p, max_new_tokens)\n",
    "    return response\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    state = gr.State([])\n",
    "    notice_markdown = \"\"\"# <center>⚔️ 大语言模型竞技场 ⚔️</center>\"\"\"\n",
    "    gr.Markdown(notice_markdown, elem_id=\"notice_markdown\")\n",
    "    \n",
    "    with gr.Row(elem_id=\"model_selector_row\"):\n",
    "        \n",
    "        model_selector = gr.Dropdown(\n",
    "            choices=models,\n",
    "            value=models[0] if len(models) > 0 else \"\",\n",
    "            interactive=True,\n",
    "            show_label=False,\n",
    "            container=False,\n",
    "        )\n",
    "        \n",
    "    chatbot = gr.Chatbot(\n",
    "        [],\n",
    "        label=\"向下滚动并开始聊天\",\n",
    "        avatar_images=((os.path.join(os.path.dirname(__file__), \"human.png\")),\n",
    "            (os.path.join(os.path.dirname(__file__), \"bot.png\"))),).style(height=350)\n",
    "\n",
    "    with gr.Row():\n",
    "        \n",
    "        with gr.Column(scale=0.85):\n",
    "            textbox = gr.Textbox(\n",
    "                show_label=False,\n",
    "                placeholder=\"请在此输入您的提示词并按Enter键:\",\n",
    "                container=False,\n",
    "                elem_id=\"input_box\"\n",
    "            )\n",
    "        \n",
    "        with gr.Column(scale=0.15, min_width=0):\n",
    "            send = gr.Button(value=\"发送\", variant=\"primary\")\n",
    "\n",
    "    with gr.Row():\n",
    "        upvote_btn = gr.Button(value=\"👍 赞成\")\n",
    "        downvote_btn = gr.Button(value=\"👎 否决\")\n",
    "        regenerate_btn = gr.Button(value=\"🔄 重新生成\")\n",
    "        clear_btn = gr.Button(value=\"🗑️ 清除历史\")\n",
    "\n",
    "    with gr.Accordion(\"Parameters\", open=False):\n",
    "   \n",
    "        temperature = gr.Slider(minimum=0.0, \n",
    "                                maximum=1.0, \n",
    "                                value=0.7,\n",
    "                                step=0.1,\n",
    "                                interactive=True,\n",
    "                                label=\"Temperature\")\n",
    "\n",
    "        top_p = gr.Slider(minimum=0.0, \n",
    "                          maximum=1.0, \n",
    "                          value=1.0,\n",
    "                          step=0.1,\n",
    "                          interactivate=True,\n",
    "                          label=\"Top p\")\n",
    "\n",
    "        max_new_tokens = gr.Slider(minimum=16, \n",
    "                                   maximum=2048, \n",
    "                                   value=512,\n",
    "                                   step=1, \n",
    "                                   interactivate=True,\n",
    "                                   label=\"Max new tokens\")\n",
    "        \n",
    "    load_model(model_selector.value)\n",
    "        \n",
    "    model_selector.change(load_model, \n",
    "                          inputs=[model_selector], \n",
    "                          outputs=[state, chatbot, textbox] + [upvote_btn, downvote_btn], \n",
    "                          show_progress=False,\n",
    "                         )\n",
    "    \n",
    "    textbox.submit(\n",
    "        chat,\n",
    "        inputs=[model_selector, textbox, state, temperature, top_p, max_new_tokens],\n",
    "        outputs=[chatbot, state] + [upvote_btn, downvote_btn],\n",
    "    )\n",
    "\n",
    "    send.click(\n",
    "        chat,\n",
    "        inputs=[model_selector, textbox, state, temperature, top_p, max_new_tokens],\n",
    "        outputs=[chatbot, state] + [upvote_btn, downvote_btn],\n",
    "    )\n",
    "    \n",
    "    upvote_btn.click(\n",
    "        upvote_last_response,\n",
    "        inputs=[state, model_selector],\n",
    "        outputs=[chatbot, state] + [upvote_btn, downvote_btn],\n",
    "    )\n",
    "    \n",
    "    downvote_btn.click(\n",
    "        downvote_last_response,\n",
    "        inputs=[state, model_selector],\n",
    "        outputs=[chatbot, state] + [upvote_btn, downvote_btn],\n",
    "    )\n",
    "    \n",
    "    regenerate_btn.click(\n",
    "        chat,\n",
    "        inputs=[model_selector, textbox, state, temperature, top_p, max_new_tokens],\n",
    "        outputs=[chatbot, state] + [upvote_btn, downvote_btn],\n",
    "    )\n",
    "        \n",
    "    clear_btn.click(\n",
    "        clear_history,\n",
    "        inputs=None,\n",
    "        outputs=[state, chatbot, textbox] + [upvote_btn, downvote_btn],\n",
    "    )\n",
    "\n",
    "demo.queue()\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
